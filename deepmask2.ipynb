{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deepmask2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrQggyPkM5LU",
        "outputId": "260e80a5-ac0d-4037-8498-fe99617d66e7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOWjLI9y8tSr",
        "outputId": "4853a503-7467-48e4-9b85-5690ac11e0e2"
      },
      "source": [
        "!git clone https://github.com/AIZOOTech/FaceMaskDetection.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'FaceMaskDetection'...\n",
            "remote: Enumerating objects: 198, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 198 (delta 11), reused 15 (delta 1), pack-reused 166\u001b[K\n",
            "Receiving objects: 100% (198/198), 57.77 MiB | 25.72 MiB/s, done.\n",
            "Resolving deltas: 100% (87/87), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDcPyjjB9hSY"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "import xml.etree.ElementTree as ET\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.optim as optim\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from skimage import io\n",
        "\n",
        "sys.path.append('/content/FaceMaskDetection/models')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HXWigqpMEGz"
      },
      "source": [
        "class MaskDataset(Dataset):\n",
        "    def __init__(self, img_root_dir, xml_root_dir, transform = None):\n",
        "      self.ids = [x[:-4] for x in os.listdir(img_root_dir)]\n",
        "      self.labels = []\n",
        "      for xml_file in os.listdir(xml_root_dir):\n",
        "        label = ET.parse(os.path.join(xml_root_dir , xml_file)).getroot().find('object').find('name').text\n",
        "        if label == 'with_mask':\n",
        "          self.labels.append(1)\n",
        "        else:\n",
        "          self.labels.append(0)\n",
        "      self.img_root_dir = img_root_dir\n",
        "      self.xml_root_dir = xml_root_dir\n",
        "      self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(r\"/content/drive/Shareddrives/deepmask/DeepMask/FaceMaskDataset/archive_2/images\", self.ids[index] + '.png')\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        height, width, _ = img.shape\n",
        "        image_resized = cv2.resize(img, (360,360))\n",
        "        image_np = image_resized / 255.0  # 归一化到0~1\n",
        "        image_exp = np.expand_dims(image_np, axis=0)\n",
        "        image_transposed = image_exp.transpose((0, 3, 1, 2))\n",
        "        input_img = torch.tensor(image_transposed).float()\n",
        "        #skimage.color.convert_colorspace()\n",
        "        y_label = torch.tensor(float(self.labels[index]))\n",
        "        return (input_img, y_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXBQAheW9VG_"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, pre_model):\n",
        "    super().__init__()\n",
        "    self.pre_model = pre_model\n",
        "  def forward(self, x):\n",
        "    x = x.squeeze()\n",
        "    x = self.pre_model(x)\n",
        "    #print(x.shape())\n",
        "    x = F.sigmoid(nn.Linear(self.pre_model.fc.in_features, 2)(x))\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoZYLtNa9-Rk"
      },
      "source": [
        "def load_data():\n",
        "  dataset = MaskDataset(img_root_dir = '/content/drive/Shareddrives/deepmask/DeepMask/FaceMaskDataset/archive_2/images',\n",
        "                        xml_root_dir = '/content/drive/Shareddrives/deepmask/DeepMask/FaceMaskDataset/archive_2/annotations',\n",
        "                        transform = None)#transforms.Compose([])#transforms.ToPILImage(),\n",
        "                                                        #transforms.Resize((256, 256)),\n",
        "                                                        #transforms.Grayscale(),\n",
        "                                                        #transforms.ToTensor()]))\n",
        "                        \n",
        "  \n",
        "  train_size = int(0.8 * len(dataset))\n",
        "  test_size = len(dataset) - train_size\n",
        "  \n",
        "  train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "  return train_loader, test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udo_tglkA536"
      },
      "source": [
        "def train(model, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  correct = 0\n",
        "  for batch_idx, (data, taget) in tqdm(enumerate(train_loader)):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.cross_entropy(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx % 100 == 0:\n",
        "      print('Train Epoch: {}\\\\tLoss: {:.6f}'.format(epoch, loss.item()))\n",
        "  accuracy = test(model, train_loader)\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqNpSJZ5ZAa6"
      },
      "source": [
        "def test(model, test_loader):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      output = model(data)\n",
        "      pred = output.argmax(dim=1, keepdim=True)\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc31WnMd-Sn3"
      },
      "source": [
        "def main():\n",
        "  \n",
        "  #num_epochs = 3\n",
        "  #learning_rate = 0.001\n",
        "  #train_loader, test_loader = load_data()\n",
        "  #model_path = '/content/FaceMaskDetection/models/model360.pth'\n",
        "  #model = torch.load(model_path)\n",
        "  #net = Net(model)\n",
        "\n",
        "  #optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
        "  #train_acc_arr, test_acc_arr = [], []\n",
        "\n",
        "  #for epoch in range(num_epochs):\n",
        "  #  train_acc = train(net, train_loader, optimizer, epoch)\n",
        "  ##  print('\\nTrain set Accuracy: {:.0f}%\\n'.format(train_acc))\n",
        "  #  test_acc = test(net, test_loader)\n",
        "  #  print('\\nTrain set Accuracy: {:.0f}%\\n'.format(test_acc))\n",
        "  #  train_acc_arr.append(train_acc)\n",
        "  #  test_acc_arr.append(test_acc)\n",
        "  #torch.save(net.state.dict(), \"deepMaskModel.pt\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77uDwvuD_DHH",
        "outputId": "00009473-d00c-4851-c718-53099b809fd1"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'MainModel.KitModel' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "0it [00:00, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z2Gs7PuTTVF"
      },
      "source": [
        "import torch\n",
        "import sys\n",
        "import cv2\n",
        "import numpy as np\n",
        "import copy\n",
        "import os\n",
        "sys.path.append(r'C:\\Users\\shrey\\hackUmass\\FaceMaskDetection\\models')\n",
        "\n",
        "model_path = '/content/FaceMaskDetection/models/model360.pth'\n",
        "\n",
        "model = torch.load(model_path)\n",
        "model.eval()\n",
        "\n",
        "for i in os.listdir(r\"/content/drive/Shareddrives/deepmask/DeepMask/FaceMaskDataset/archive_2/images\"):\n",
        "    img_path = os.path.join(r\"/content/drive/Shareddrives/deepmask/DeepMask/FaceMaskDataset/archive_2/images\", i)\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    height, width, _ = img.shape\n",
        "    image_resized = cv2.resize(img, (360,360))\n",
        "    image_np = image_resized / 255.0  # 归一化到0~1\n",
        "    image_exp = np.expand_dims(image_np, axis=0)\n",
        "    image_transposed = image_exp.transpose((0, 3, 1, 2))\n",
        "    input_img = torch.tensor(image_transposed).float()\n",
        "    print(input_img.shape)\n",
        "    a,b = model.forward(input_img)\n",
        "    a = a.detach()\n",
        "    b = b.detach()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y48zfSb4vgFg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}